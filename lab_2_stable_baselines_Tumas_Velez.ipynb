{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DiploDatos/AprendizajePorRefuerzos/blob/master/lab_2_stable_baselines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SytTR8K1oII-"
   },
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BFrF74foIJD"
   },
   "source": [
    "Créditos:\n",
    "\n",
    "* Documentación y repo de Stable-baselines https://stable-baselines3.readthedocs.io.\n",
    "    * Tutorial sobre SB3: https://github.com/araffin/rl-tutorial-jnrr19.\n",
    "* Documentación y repo de Gymnasium https://github.com/Farama-Foundation/Gymnasium. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlZ0ehGZoIJD"
   },
   "source": [
    "Stable-baselines3: framework de deep RL que provee interfaces para ejecutar y adaptar algoritmos de RL \"al estilo scikit-learn\". Permite utilizar agentes abstrayéndonos de los detalles de bajo nivel de abstracción referentes a la implementación del algoritmo$^1$\n",
    "\n",
    "Además, ofrece herramientas muy útiles como\n",
    "\n",
    "* Monitores que permiten ver el rendimiento del agente según se desempeña en el entorno, sin tener que esperar a que finalice de entrenar.\n",
    "* Callbacks que permiten accionar eventos cuando se cumplen algunas condiciones en el entrenamiento de nuestro agente (por ejemplo, detenerlo si la recompensa recibida es menor a cierto umbral tras un cierto período de tiempo).\n",
    "\n",
    "\n",
    "Documentación https://stable-baselines3.readthedocs.io\n",
    "\n",
    "Es un fork activamente mantenido de [OpenAI baselines](https://github.com/openai/baselines)\n",
    "\n",
    "La versión 3 cambia el framework subyacente de Tensorflow a Pytorch y está activamente en desarrollo; no obstante la versión 2 es completamente funcional\n",
    "\n",
    "$^1$ no obstante, al igual que sucede generalmente con librerías de ML: \n",
    "\n",
    "* Siempre es bueno tener en mente las características, ventajas y desventajas del algoritmo utilizado, pues de eso depende mucho la convergencia de nuestra solución, especialmente cuando se emplean entornos adaptados para nuestras necesidades. \n",
    "\n",
    "* Esta librería, al igual que demás frameworks generales de RL, están muy probadas en entornos estándares de RL como Atari o PyBullet. No obstante, es posible que nuestro entorno o nuestras necesidades difieran significativamente, lo que hace que en algunos casos haya que meter mano directo en el código de los algoritmos/librería."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4SWX4wuoIJE"
   },
   "source": [
    "# Interfaz básica stable-baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD-FSSuKoIJF"
   },
   "source": [
    "### Instalación de Stable-baselines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttqOLol_oIJG",
    "outputId": "e7b26d16-a6d3-45af-ca5f-332a2330427a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Instalación (no modificar)\n",
    "\n",
    "# Estamos en Colab?\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde Windows, además, instalar: \n",
    "* Microsoft Visual C++ desde https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
    "* PyType, mediante `conda install -c conda-forge pytype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUUkScRxoIJQ"
   },
   "source": [
    "### Instalación de RLBaselinesZoo (Opcional!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiKPJUfdoIJQ"
   },
   "source": [
    "Desde Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZM_pM0mIoIJQ",
    "outputId": "8b3aabec-28d0-4caf-93e7-4f4d6cd8725f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Instalación de RLBaselinesZoo (no modificar)\n",
    "\n",
    "# Instalamos esta lib opcional?\n",
    "install_rlzoo = False\n",
    "\n",
    "if IN_COLAB and install_rlzoo:\n",
    "    !pip install rl_zoo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzBEoyPzoIJR"
   },
   "source": [
    "Desde Linux, ejecutando\n",
    "\n",
    "    pip install rl_zoo3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPUpOonyoIJH"
   },
   "source": [
    "## Ejecución de un algoritmo de RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Morjd1dFoIJH"
   },
   "source": [
    "### Importaciones/inicializaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OG7i44kqoIJH",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "#!pip install stable-baselines3[extra]\n",
    "#!pip install rl_zoo3\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "\n",
    "os.makedirs('logs', exist_ok=True)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is2ytf-coIJH"
   },
   "source": [
    "### Ejemplo básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h8SqFh7noIJI",
    "outputId": "1d56db34-53bd-450a-8fe8-825f7201ffcf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7feeca32ab50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# MlpPolicy es una política \"estándar\" que aprende con perceptron multicapa\n",
    "# (es decir sin capas convolucionales o demás variantes),\n",
    "# 2 capas ocultas con 64 neuronas cada una\n",
    "model = DQN(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0f5JFC7AoIJK"
   },
   "source": [
    "### Renderización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5nIL0TuwoIJK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nacho/anaconda3/envs/jupyter/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:215: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "if not IN_COLAB:\n",
    "    obs, info = env.reset(seed=42)\n",
    "    for i in range(1000):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "\n",
    "        terminated = terminated or truncated\n",
    "        if terminated:\n",
    "            obs, info = env.reset()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTtXPFd2oIJK"
   },
   "source": [
    "### Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFYyMrvboIJK"
   },
   "source": [
    "#### Ver rendimiento del agente en tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pC1hqla2oIJK",
    "outputId": "77c1b83c-9952-4ee7-b4eb-2c8bfa0d2024"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fedd0995a50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\"MlpPolicy\", gym.make(\"CartPole-v1\"), tensorboard_log=\"tensorboard/\")\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rldIL1u4oIJL"
   },
   "source": [
    "Para verlo en tensorboard, correr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v4Db-mCxoIJL"
   },
   "source": [
    "`tensorboard --logdir=tensorboard/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2AfqEmCWxWFv",
    "outputId": "e2297f30-400f-4480-b04e-90f1b4dc2549"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3154bd010f61aab3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3154bd010f61aab3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u0_1dk5oIJL",
    "tags": []
   },
   "source": [
    "### Monitor\n",
    "\n",
    "Vamos a crear un monitor para loguear nuestro agente en la carpeta logs. Nuestro monitor guardará datos de recompensa (r), duración (l) y tiempo total (t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7uF7sQdoIJL",
    "outputId": "ea0fb222-58aa-4c18-9c8c-ae97132ee3ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fedd09b3090>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = Monitor(env, \"logs/\")  # reemplazamos env por su monitor\n",
    "\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    ")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yVVSGjvoIJM"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47XdjvaSoIJM",
    "outputId": "8eb9dd8d-ca66-4aee-cdbf-bce8a510ecea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=9.40 +/- 0.80\n",
      "Episode length: 9.40 +/- 0.80\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=9.80 +/- 0.75\n",
      "Episode length: 9.80 +/- 0.75\n",
      "New best mean reward!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fedd0790410>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "callbacks = []  # lista de callbacks a usar, pueden ser varios\n",
    "\n",
    "# callback para detener entrenamiento al alcanzar recompensa de 9.8\n",
    "# (es una recompensa muy baja, pero la establecemos a fines demostrativos)\n",
    "stop_training_callback = StopTrainingOnRewardThreshold(reward_threshold=9.8)\n",
    "\n",
    "# al crear EvalCallback, se asocia el mismo con stop_training_callback\n",
    "callbacks.append(\n",
    "    EvalCallback(\n",
    "        Monitor(env, \"logs/\"),\n",
    "        eval_freq=1000,\n",
    "        callback_on_new_best=stop_training_callback,\n",
    "    )\n",
    ")\n",
    "\n",
    "# la semilla aleatoria hace que las ejecuciones sean determinísticas\n",
    "model = DQN(\"MlpPolicy\", env, seed=42)\n",
    "model.learn(total_timesteps=10000, callback=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCPD7oaNoIJM"
   },
   "source": [
    "### Ejecutar agente RL en múltiples ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNiTT2AzoIJM"
   },
   "source": [
    "Esta librería provee una interfaz para ejecutar agentes en varias instancias de un mismo entorno a la vez (*vectorized environments*), de modo tal que se habilite la ejecución paralela y de otras funcionalidades útiles.\n",
    "\n",
    "Para ello, varios de sus algoritmos implementan cambios que consideren la posibilidad de que haya múltiples entornos subyacentes, por ejemplo `step(accion)` cambia a `step(lista_acciones)`, aplicando acciones a todos los entornos, recibiendo ahora múltiples observaciones y recompensas.\n",
    "\n",
    "Otro cambio: se aplica `reset()` automáticamente a cada entorno que llega a un estado final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GGZjjcDoIJN"
   },
   "source": [
    "SB brinda dos formas de utilizar entornos vectorizados:\n",
    "\n",
    "* **DummyVecEnv**, el cuál consiste en un *wrapper* de varios entornos, los cuáles funcionarán en un sólo hilo. Este wrapper es útil como entrada de algoritmos que requieren los entornos de esta forma, y habilita los procesamientos y operaciones comunes de los entornos vectorizados.\n",
    "* **SubprocVecEnv**, el cuál paraleliza multiples entornos pero en procesos de ejecucíon separados. Cada proceso tiene su propia memoria y puede adquirir derechos sobre las CPUs de la computadora donde se ejecuta. Se utiliza cuando el entorno del agente es computacionalemente complejo. Atención! **Puede comer mucha RAM**.\n",
    "\n",
    "Vemos un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovc6JDmYoIJN",
    "outputId": "6f36d2d0-bc4e-4299-b612-58103bc73f6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fedd09ab4d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ejemplo de ambiente dummy, donde creamos 4 instancias del mismo\n",
    "venv = DummyVecEnv([lambda: gym.make(\"CartPole-v1\")] * 4)\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv,\n",
    ")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjH4ZQ4NoIJO"
   },
   "source": [
    "### Ejecutar agente con políticas personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jp_aPhIdoIJO",
    "outputId": "19b3f2a7-2821-4fe9-a749-b32be258aee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'CartPole-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 1503     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 28.6         |\n",
      "|    ep_rew_mean          | 28.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 715          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085145775 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.686       |\n",
      "|    explained_variance   | -0.00709     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0177      |\n",
      "|    value_loss           | 36.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 37.8        |\n",
      "|    ep_rew_mean          | 37.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009379704 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0219     |\n",
      "|    value_loss           | 34          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 51.8        |\n",
      "|    ep_rew_mean          | 51.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 783         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013218449 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.622      |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    value_loss           | 44.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 68.2       |\n",
      "|    ep_rew_mean          | 68.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 799        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01732843 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.594     |\n",
      "|    explained_variance   | 0.316      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22.7       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 50.2       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fedcc453690>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una clase con una red neuronal de 128x128 neuronas\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\", \n",
    "    policy_kwargs=dict(net_arch=[128, 128]), \n",
    "    env=\"CartPole-v1\", \n",
    "    verbose=1\n",
    ")\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Buen post](https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2) en donde se explica qué significan varias de estas métricas. Para verlas en detalle podemos consultar directamente el [código fuente](https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py), que está bien documentado y no es muy difícil de seguir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbwkBi87oIJO"
   },
   "source": [
    "### Utilizar un entorno personalizado\n",
    "\n",
    "Antes que nada, además de la interfaz que ya vimos de Gym, hay otras nociones que tenemos que tener en cuenta en este contexto:\n",
    "\n",
    "* Los entornos definen un espacio de estados y de acciones, a partir de los cuáles los modelos asumen y respetan la \"forma\" de observaciones y acciones. Por ejemplo, algunos algoritmos están diseñados para espacios de acciones discretos (DQN), continuos (DDPG) o bien poseen implementaciones particulares pueden usarse en ambos (PPO, en el repo de SB3). En cuanto a los espacios, algunos algoritmos asumen explícitamente un espacio discreto (y pequeño), como Q-Learning, mientras que otros como PPO asumen cualquier tipo de espacio.\n",
    "* Los dos tipos más comunes de estados o acciones son los espacios discretos `gym.spaces.Discrete` y los continuos `gym.spaces.Box`.\n",
    "* Los espacios discretos definen un conjunto de $n$ estados/acciones $\\{ 0, 1, \\dots, n-1 \\}$, mientras que los espacios continuos definen un espacio $\\mathbb{R}^d$, de una de las siguientes 4 formas: $[a, b], (-\\infty, b], [a, \\infty), (-\\infty, \\infty)$, en donde $a,b$ son las cotas superior e inferior (de existir).\n",
    "* Ejemplos: un espacio de acciones `Discrete(4)` tiene 4 acciones: $\\{0,1,2,3\\}$; un espacio de estados `Discrete(16)` tiene 16 estados. Un espacio de estados ALTURA, ANCHO, N_CANALES que represente una imagen RGB acotada en $[a=0, b=255]$ se puede crear como\n",
    "\n",
    "`observation_space = spaces.Box(low=0, high=255, shape=(HEIGHT, WIDTH, N_CHANNELS), dtype=np.uint8)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFLZieLNoIJO"
   },
   "source": [
    "Para usar un entorno compatible por esta librería, el mismo tiene que heredar de *gym.Env*. Vemos un ejemplo (crédito: Antonin Raffin https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/5_custom_gym_env.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "str0v6DUoIJO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class GoLeftEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Ambiente personalizado que sigue la interfaz de gym.\n",
    "    Es un entorno simple en el cuál el agente debe aprender a ir siempre\n",
    "    hacia la izquierda.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dado que podemos estar en colab, no podemos implementar la salida\n",
    "    # por interfaz gráfica ('human' render mode)\n",
    "    metadata = {\"render.modes\": [\"console\"]}\n",
    "    # Definimos las constantes\n",
    "    LEFT = 0\n",
    "    RIGHT = 1\n",
    "\n",
    "    def __init__(self, grid_size=10):\n",
    "        super(GoLeftEnv, self).__init__()\n",
    "\n",
    "        # Tamaño de la grilla de 1D\n",
    "        self.grid_size = grid_size\n",
    "        # Inicializamos en agente a la derecha de la grilla\n",
    "        self.agent_pos = grid_size - 1\n",
    "\n",
    "        # Definimos el espacio de acción y observaciones\n",
    "        # Los mismos deben ser objetos gym.spaces\n",
    "        # En este ejemplo usamos dos acciones discretas: izquierda y derecha\n",
    "        n_actions = 2\n",
    "        self.action_space = spaces.Discrete(n_actions)\n",
    "        # La observación será la coordenada donde se encuentra el agente\n",
    "        # puede ser descrita tanto por los espacios Discrete como Box\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None) -> Tuple[np.array, dict]:\n",
    "        \"\"\"\n",
    "        Reinicia el ambiente y devuelve la observación inicial\n",
    "\n",
    "        Importante: la observación devuelta debe ser un array de numpy\n",
    "        :return: (np.array, dict)\n",
    "        \"\"\"\n",
    "        # Se inicializa el agente a la derecha de la grilla\n",
    "        self.agent_pos = self.grid_size - 1\n",
    "\n",
    "        # convertimos con astype a float32 (numpy) para hacer más general\n",
    "        # el agente (en caso de que querramos usar acciones continuas)\n",
    "        return (np.array([self.agent_pos]).astype(np.float32), {})\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.LEFT:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == self.RIGHT:\n",
    "            self.agent_pos += 1\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Se recibió una acción inválida={action} que no es parte del\\\n",
    "                    espacio de acciones\"\n",
    "            )\n",
    "\n",
    "        # Evitamos que el agente se salga de los límites de la grilla\n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
    "\n",
    "        # Llegó el agente a su estado objetivo (izquierda) de la grilla?\n",
    "        terminated = bool(self.agent_pos == 0)\n",
    "        truncated = False  # no limitamos la duración de los episodios\n",
    "\n",
    "        # Asignamos recompensa sólo cuando el agente llega a su objetivo\n",
    "        # (recompensa = 0 en todos los demás estados)\n",
    "        reward = 1 if self.agent_pos == 0 else 0\n",
    "\n",
    "        # gym también nos permite devolver información adicional, ej. en Atari:\n",
    "        # las vidas restantes del agente (no usaremos esto por ahora)\n",
    "        info = {}\n",
    "\n",
    "        return (\n",
    "            np.array([self.agent_pos]).astype(np.float32),\n",
    "            reward,\n",
    "            terminated,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        if mode != \"console\":\n",
    "            raise NotImplementedError()\n",
    "        # en nuestra interfaz de consola, representamos el agente como una\n",
    "        # cruz, y el resto como un punto\n",
    "        print(\".\" * self.agent_pos, end=\"\")\n",
    "        print(\"x\", end=\"\")\n",
    "        print(\".\" * (self.grid_size - self.agent_pos))\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validamos que el ambiente cumpla con la interfaz de gym\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "env = GoLeftEnv()\n",
    "# Si el entorno no cumple con la interfaz, se lanzará una excepción\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "flB9-j1SoIJP",
    "outputId": "67ccb533-c91c-441a-8f85-72346c10e1ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | 1        |\n",
      "| time/              |          |\n",
      "|    fps             | 1607     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 55.5        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 976         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016817436 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.677      |\n",
      "|    explained_variance   | -0.416      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    value_loss           | 0.0233      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 24.3        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 951         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024211025 |\n",
      "|    clip_fraction        | 0.418       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.621      |\n",
      "|    explained_variance   | 0.384       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00368     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0491     |\n",
      "|    value_loss           | 0.0223      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.3        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 943         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038395934 |\n",
      "|    clip_fraction        | 0.394       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.509      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0857     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0504     |\n",
      "|    value_loss           | 0.0097      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 11.6       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 938        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06678659 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.333     |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.012     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    value_loss           | 0.0024     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 10.4        |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 967         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013063651 |\n",
      "|    clip_fraction        | 0.08        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0424     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 0.000492    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 10.3         |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 987          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059565734 |\n",
      "|    clip_fraction        | 0.0691       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.173       |\n",
      "|    explained_variance   | 0.725        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0126      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0207      |\n",
      "|    value_loss           | 0.000257     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9.16       |\n",
      "|    ep_rew_mean          | 1          |\n",
      "| time/                   |            |\n",
      "|    fps                  | 966        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09140915 |\n",
      "|    clip_fraction        | 0.0606     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.0529    |\n",
      "|    explained_variance   | 0.774      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0284    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0247    |\n",
      "|    value_loss           | 0.000216   |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 9.04          |\n",
      "|    ep_rew_mean          | 1             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 983           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020912211 |\n",
      "|    clip_fraction        | 0.00371       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0194       |\n",
      "|    explained_variance   | 0.972         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -0.0117       |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.00287      |\n",
      "|    value_loss           | 1.94e-05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 9.02          |\n",
      "|    ep_rew_mean          | 1             |\n",
      "| time/                   |               |\n",
      "|    fps                  | 996           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018037166 |\n",
      "|    clip_fraction        | 0.00278       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -0.0145       |\n",
      "|    explained_variance   | 0.977         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | -1.85e-05     |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.00239      |\n",
      "|    value_loss           | 1.48e-05      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = GoLeftEnv(grid_size=10)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1).learn(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos agente entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........x.\n",
      "Box(0.0, 10.0, (1,), float32)\n",
      "Discrete(2)\n",
      "1\n",
      "Step 1\n",
      "obs= [8.] reward= 0 done= False\n",
      "........x..\n",
      "Step 2\n",
      "obs= [7.] reward= 0 done= False\n",
      ".......x...\n",
      "Step 3\n",
      "obs= [6.] reward= 0 done= False\n",
      "......x....\n",
      "Step 4\n",
      "obs= [5.] reward= 0 done= False\n",
      ".....x.....\n",
      "Step 5\n",
      "obs= [4.] reward= 0 done= False\n",
      "....x......\n",
      "Step 6\n",
      "obs= [3.] reward= 0 done= False\n",
      "...x.......\n",
      "Step 7\n",
      "obs= [2.] reward= 0 done= False\n",
      "..x........\n",
      "Step 8\n",
      "obs= [1.] reward= 0 done= False\n",
      ".x.........\n",
      "Step 9\n",
      "obs= [0.] reward= 1 done= True\n",
      "x..........\n",
      "Objetivo cumplido! reward=1\n"
     ]
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "env.render()\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())\n",
    "\n",
    "GO_LEFT = 0\n",
    "# Agent óptimo: siempre va hacia la izquierda!\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "    print(f\"Step {step + 1}\")\n",
    "    obs, reward, terminated, truncated, info = env.step(GO_LEFT)\n",
    "    done = terminated or truncated\n",
    "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"Objetivo cumplido!\", f\"reward={reward}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgrWKJlsoIJT"
   },
   "source": [
    "## Normalización de features y recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIfn3o99oIJT"
   },
   "source": [
    "Stable-Baselines3 tiene predefinidos un conjunto de **wrappers** genericos que pueden utilizarse para preprocesar las observaciones que llegan al agente RL, desacoplando del mismo el prepocesamiento.\n",
    "\n",
    "Entre las funcionalidades disponibles tenemos:\n",
    "* **VecFrameStack**: Se utiliza cuando la observación que percibe el agente es una imagen. Sirve para expandir el espacio de estados apilando N frames de manera conjunta (ver [paper de DQN](https://arxiv.org/abs/1312.5602) para más detalle).\n",
    "* **VecNormalize**: Se utiliza para normalizar las observaciones y/o las recompenzas que percibe el agente, a $\\mu=0$ y $\\sigma=1$. También permite cortar valores de observaciones y/o recompensas que excedan un rango establecido. \n",
    "* **VecCheckNan**: Se utiliza para trackear los estados del entorno que generan que los gradientes de la NN se hagan NaN.\n",
    "* **VecVideoRecorder**: Se utiliza para exportar el funcionamiento de la política aprendida por el agente a un video (MP4).\n",
    "\n",
    "Ademas, se pueden crear **wrappers** personalizados extendiendo la clase **VecEnvWrapper**:\n",
    "\n",
    "```\n",
    "class MiWrapper(VecEnvWrapper):\n",
    "    [...]\n",
    "```\n",
    "### Ejemplo (VecNormalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VOh1HQCVJNA",
    "outputId": "b9f70740-a806-4978-988d-aa296af53c58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fedcc49ced0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "# Multiples entornos vectorizados. Normalizamos tanto observaciones como\n",
    "# recompensas, y recortamos ambas cuando exceden 10\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize(\n",
    "    env, norm_obs=True, norm_reward=True, clip_obs=10.0, clip_reward=10.0\n",
    ")\n",
    "\n",
    "model = DQN(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exel4Gr8oIJQ",
    "tags": []
   },
   "source": [
    "# RL-baselines3-zoo\n",
    "\n",
    "Colección de agentes RL y herramientas útiles para ejecutarlos, evaluarlos e incluso hacer videos con ellos. Los agentes de este repo están preparados con la configuración requerida para los distintos tipos de entornos, incluyendo Atari, PyBullet y entornos clásicos, incluyendo configuraciones e híper-parámetros que producen buenas políticas para tales entornos.\n",
    "\n",
    "Esta librería ofrece un muy buen punto de partida para utilizar agentes / entornos personalizados, ya que ofrece una [interfaz](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/train.py) fácilmente adaptable a nuestras necesidades.\n",
    "\n",
    "Si se usan entornos personalizados con rl-baselines3-zoo, debe tenerse en cuenta que se deben definir todos los híper-parámetros de antemano, sea al instanciar el agente, o en la carpeta */rl-baselines3-zoo/hyperparams*; de lo contrario arrojará error por no encontrar qué híper-parámetro usar.\n",
    "\n",
    "**Importante:** esta librería es enteramente opcional, no es necesaria para realizar los labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    import rl_zoo3\n",
    "    zoo_installed = True\n",
    "except ImportError as e:\n",
    "    zoo_installed = False\n",
    "\n",
    "zoo_installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6TCyu_UoIJR"
   },
   "source": [
    "## Ejecución\n",
    "\n",
    "Los agentes pueden ser llamados desde la consola mediante comandos como\n",
    "\n",
    "`python train.py --algo algo_name --env env_id`\n",
    "\n",
    "Los cuales pueden ser llamados usando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jBLMVxhIoIJR"
   },
   "outputs": [],
   "source": [
    "if zoo_installed:\n",
    "    # lo siguiente es equivalente a ejecutar en la terminal:\n",
    "    # python -m rl_zoo3.train --algo ppo --env CartPole-v1\n",
    "\n",
    "    args = [\"--algo\", \"ppo\", \"--env\", \"CartPole-v1\"]\n",
    "\n",
    "    p = Popen(\n",
    "        [\"python\", \"-m\", \"rl_zoo3.train\"] + args, stdin=PIPE, stdout=PIPE, stderr=PIPE\n",
    "    )\n",
    "    output, err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    os.chdir(cwd)\n",
    "    assert rc == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40H4hNcqoIJR"
   },
   "source": [
    "Ver en acción el agente entrenado (nota: no disponible en Google Colab, requiere ffmpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "cNija96loIJR"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB and zoo_installed:\n",
    "    # lo siguiente es equivalente a ejecutar en la terminal:\n",
    "    # python -m rl_zoo3.enjoy --algo ppo --env CartPole-v1 --folder logs/\n",
    "\n",
    "    args = [\"--algo\", \"ppo\", \"--env\", \"CartPole-v1\", \"--folder\", \"logs/\"]\n",
    "\n",
    "    p = Popen(\n",
    "        [\"python\", \"-m\", \"rl_zoo3.enjoy\"] + args, stdin=PIPE, stdout=PIPE, stderr=PIPE\n",
    "    )\n",
    "    output, err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    os.chdir(cwd)\n",
    "    assert rc == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSfIitK7oIJR"
   },
   "source": [
    "También es posible grabar un video! Ver https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#record-a-video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phq975BYoIJR"
   },
   "source": [
    "Ver curva de aprendizaje obtenida por el agente desde *utils.plot*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Q8EA41oQoIJS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen runpy>:128: RuntimeWarning: 'rl_zoo3.plots.plot_train' found in sys.modules after import of package 'rl_zoo3.plots', but prior to execution of 'rl_zoo3.plots.plot_train'; this may result in unpredictable behaviour\n"
     ]
    }
   ],
   "source": [
    "# equivalente a ejecutar en la terminal:\n",
    "# python -m rl_zoo3.plots.plot_train --algo ppo --env CartPole-v1 --exp-folder logs/\n",
    "#!pip install rl_zoo3\n",
    "args = [\"--algo\", \"ppo\", \"--env\", \"CartPole-v1\", \"--exp-folder\", \"logs/\"]\n",
    "\n",
    "p = Popen([\"python\", \"-m\", \"rl_zoo3.plots.plot_train\"] + args, stdout=PIPE)\n",
    "output, err = p.communicate()\n",
    "rc = p.returncode\n",
    "os.chdir(cwd)\n",
    "\n",
    "assert rc == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Figure(640x480)\\n'\n"
     ]
    }
   ],
   "source": [
    "print(output) # TODO decode this output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWzAHj6qoIJT",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Híper-parámetros\n",
    "\n",
    "rl-baselines-zoo provee híper-parámetros que resultan en curvas de aprendizaje que convergen en buena cantidad de entornos. Estos híper-parámetros pueden verse en cada uno de los archivos YAML de cada algoritmo, [acá](https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams).\n",
    "\n",
    "También provee funcionalidad para optimizar los híper-parámetros con la librería [Optuna]( https://github.com/optuna/optuna). En los mismos se incluyen rangos de híper-parámetros que se usaron para optimizar entornos como los de PyBullet, y son fácilmente modificables para adaptarlo a nuestros propios entornos. Para ver cómo se llama a la interfaz de Optuna ver [este código](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/utils/hyperparams_opt.py).\n",
    "\n",
    "Nota: **Optuna come muchos recursos!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando otros agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Algunos conjuntos de entornos\n",
    "\n",
    "CartPole es una excelente línea base (de hecho suele ser la prueba preliminar de todo nuevo algoritmo), porque tiene recompensas constantes pero requiere cierta solidez por parte del algoritmo para hacerlo converger al óptimo.\n",
    "\n",
    "No obstante, al implementar un algoritmo, es deseable que el mismo pueda desenvolverse de forma consistente en varios grupos de entornos. A continuación va una lista con varios entornos que sirven como prueba:\n",
    "\n",
    "| Entornos                                                                                                           | Estados            | Acciones            | Dificultad      | Implementado por                                                                                                                                                            |\n",
    "|--------------------------------------------------------------------------------------------------------------------|--------------------|---------------------|-----------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Clásicos (CartPole, MountainCar, Pendulum, Deep sea (testea exploración), umbrella (testea asignación de crédito)) | Discretos/Continuos          | Continuas/discretas | Baja/Media      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) y [BSuite](https://github.com/deepmind/bsuite)                                                                                                                          |\n",
    "| Grilla (desde pequeñas donde hay que salir hasta grandes con muchas habitaciones y subproblemas)                   | Discretos/imágenes | Discretas           | Baja/Media/Alta | [gym-minigrid](https://github.com/maximecb/gym-minigrid)                                                                                                                    |\n",
    "| Grilla/plataforma/estilo Atari, generados proceduralmente                                                          | Imágenes           | Discretas           | Media/Alta      | [Gym](https://github.com/openai/procgen)                                                                                                                                    |\n",
    "| Plataforma 2D, como LunarLander o BipedalWalker                                                                    | Continuos          | Continuas/discretas | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments)                                                                                                                                                                         |\n",
    "| Primera persona en 3D                                                                                              | Imágenes           | Continuas/discretas | Media/Alta      | [Deepmind](https://github.com/deepmind/lab)                                                                                                                                 |\n",
    "| Simulación física de pequeños robots                                                                               | Continuos          | Continuas           | Media/Alta      | Gym (con el motor [MuJoCo](https://mujoco.org/) o [PyBullet](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.wz5to0x8kqmr)) |\n",
    "| Atari                                                                                                              | Continuos          | Discretas           | Media/Alta      | [Gym](https://github.com/openai/gym/wiki/Table-of-environments) (son todos aquellos entornos que terminan en \"*-v4*\"                                                                                                                                                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resumen de algunos algoritmos\n",
    "\n",
    "Se resumen ahora varios algoritmos del estado del arte de aprendizaje por refuerzos profundo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Algoritmo | Tipo       | Espacio de acciones | Resumen rápido                                                                                                                                                | Artículo                         |\n",
    "|-----------|------------|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------|\n",
    "| DQN       | Off-policy | Discretas           | Extiende Q-Learning a deep learning. En Stable-baselines, DQN incluye todas las mejoras ya incorporadas.                                                      | https://arxiv.org/abs/1312.5602  |\n",
    "| ACER      | Off-policy | Discretas           | Combina una arquitectura actor-critic con un buffer y repetición de experiencia.                                                                              | https://arxiv.org/abs/1611.01224 |\n",
    "| A3C       | On-policy  | Ambos               | Múltiples agentes corriendo en múltiples instancias del ambiente, acumulando sus gradientes y actualizándolos tras un cierto tiempo.                          | https://arxiv.org/abs/1602.01783 |\n",
    "| PPO       | On-policy  | Ambos               | Punta de lanza de policy gradient, incluye mecanismo para que el gradiente actualice de forma acotada, mejorando drásticamente la estabilidad.             | https://arxiv.org/abs/1707.06347 |\n",
    "| DDPG      | Off-policy | Continuas           | Como en espacios de acciones continuos es muy difícil encontrar $\\max_a Q(s,a)$, se aproxima via $Q(s, a(s \\mid \\theta_a))$, siendo $a$ un actor estocástico. | https://arxiv.org/abs/1509.02971 |\n",
    "| TD3       | Off-policy | Continuas           | Mejora DDPG utilizando dos funciones $Q$ y retrasando la actualización para reducir la sobreestimación de $Q$.                                                | https://arxiv.org/abs/1802.09477 |\n",
    "| SAC       | Off-policy | Continuas           | Usa dos funciones $Q$, introduce el bonus por entropía y usa un actor estocástico que muestrea acciones según una política $\\pi$.                             | https://arxiv.org/abs/1801.01290 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Resumen de algunas herramientas/trucos comúnmente usados\n",
    "\n",
    "* Experience replay/buffer de experiencia: guarda las experiencias en un buffer para poder usarlas repetidamente durante el entrenamiento.\n",
    "    * Ventajas: experiencias raras pero muy relevantes (ej: que tienen mucho error de actualización) quedan guardadas en memoria, pudiendo ser usadas repetidamente para aprender sin necesidad de esperar a que se repitan.\n",
    "    \n",
    "    En métodos de gradiente de política, en cambio, el aprendizaje queda reflejado en los pesos, lo cuál puede hacer que un agente no se desenvuelva correctamente en entornos de recompensa escasa como MountainCar.\n",
    "    \n",
    "    * Desventajas: requiere considerable RAM, son usables solamente en algoritmos off-policy y su muestreo no necesariamente refleja la probabilidad real de tener esas experiencias en el entorno (añadiendo sesgo), por lo que es recomendable usarlo junto con importance sampling.\n",
    "    \n",
    "* Importance sampling: aplica un descuento a las actualizaciones a partir de experience replay relacionado a cuán probable era realizar esa transición.\n",
    "* Entropía: añade un bonus a la función de recompensa para que bonifique políticas $\\pi(s \\mid a)$ que tengan mayor entropía que otras, motivando la exploración del agente.\n",
    "* Juntar varias secuencias de imágenes. Usado principalmente en entornos de Atari para poder evaluar la dirección de movimientos.\n",
    "* Normalización de recompensas/estados. Normaliza las recompensas y observaciones usualmente con una media móvil, de modo tal que las observaciones/recompensas reflejen su relación y varianza con respecto a las demás.\n",
    "* Clipping (recorte) de recompensas. Se usaba principalmente en entornos de Atari con algoritmos tipo DQN para recortar el impacto que las distintas recompensas tenían, a una constante (ej: 1). Suele usarse como una cota máxima de recompensas/observaciones normalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Lab 2\n",
    "\n",
    "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. \n",
    "\n",
    "    Algunas ideas:\n",
    "\n",
    "    * Transformar GoLeftEnv en una grilla 2D, añadir paredes / trampas / agua.\n",
    "    * Crear un entorno que juegue a algún juego como el ta-te-ti.\n",
    "    * Crea un entorno totalmente nuevo que sea de tu interés!\n",
    "\n",
    "2. Entrena agentes en entornos más complejos con librerías de agentes más avanzadas. La idea es que den rienda suelta a su creatividad. Algunas ideas de agentes / entornos:\n",
    "\n",
    "* stable-baselines/rl-baselines-zoo (como lo vimos en este notebook).\n",
    "* (CleanRL)[https://github.com/vwxyzjn/cleanrl]\n",
    "* Pueden inspirarse en [DQN](https://huggingface.co/learn/deep-rl-course/unit3/hands-on), [Policy Gradient](https://huggingface.co/learn/deep-rl-course/unit4/hands-on) o [demás secciones](https://huggingface.co/learn/deep-rl-course/unit0/introduction) del curso de RL de HuggingFace.\n",
    "* Alguna aplicación o notebook usando [Transformer RL](https://github.com/huggingface/trl).\n",
    " \n",
    "\n",
    "Tener en cuenta:\n",
    "\n",
    "* Google Colab tiene una limitante en cuanto a cantidad de recursos de CPU/GPU (incluido un \"rendimiento decreciente silencioso\"), lo cuál reduce la capacidad de entrenar distintos entornos.\n",
    "* Si el entorno no está implementado en stable-baselines, debe hacerse un wrapper a mano, lo que puede ser sencillo o puede llevar algo más de trabajo, teniendo que tocar código subyacente de la librería. \n",
    "\n",
    "\\* pueden ser usando los agentes de stable-baselines, de rl-baselines-zoo, o bien utilizando algún otro algoritmo (incluso tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5YbRAbyoIJT"
   },
   "source": [
    "# Recursos adicionales\n",
    "\n",
    "* [Excelente curso de HuggingFace para aprender más de deep RL](https://huggingface.co/learn/deep-rl-course)\n",
    "* [Framework adicional de aprendizaje por refuerzos a gran escala](https://docs.ray.io/en/master/rllib.html)\n",
    "* [Awesome Deep RL](https://github.com/kengz/awesome-deep-rl)\n",
    "* [Comunidad de Bots de RL para Rocket League](https://rlbot.org/)\n",
    "* [Blog de Lilian Weng de Deep RL, robótica y NLP](https://lilianweng.github.io/lil-log/)\n",
    "* [The Nuts and Bolts of Deep RL Research](http://joschu.net/docs/nuts-and-bolts.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por comodidad, decidimos responder las consignas aquí:\n",
    "\n",
    "\n",
    "1. Crear tu propio entorno y entrenar agentes RL en el mismo. Analizar la convergencia con distintos algoritmos* (ej: PPO, DQN), resultados con distintas funciones de recompensa e híper-parámetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nombre del Entorno: Laberinto\n",
    "\n",
    "Descripción: Un entorno de tipo laberinto en forma de cuadrícula 2D (matriz 8x8)) en el que un agente (ratón) debe encontrar una ruta desde su posición inicial (arriba izquierda) hasta un objetivo (derecha abajo) en forma de queso. El laberinto contiene obstáculos.\n",
    "\n",
    "Objetivo: El objetivo del agente es aprender una política que le permita navegar por el laberinto desde su posición inicial hasta la celda de queso (objetivo) de la manera más eficiente posible, minimizando las penalizaciones y maximizando las recompensas.\n",
    "\n",
    "Espacio de Observación: El entorno proporciona una observación que es la posición actual del agente en la cuadrícula (coordenadas x, y). Esta observación es un espacio de tipo \"Box\" con valores continuos.\n",
    "\n",
    "Espacio de Acción: El espacio de acción es discreto y permite al agente moverse hacia arriba, abajo, izquierda o derecha para navegar por la cuadrícula.\n",
    "\n",
    "Recompensas:\n",
    "Cada movimiento entre celdas adyacentes cuesta -0.04 puntos para incentivar al agente a llegar al objetivo de la manera más directa.\n",
    "Al alcanzar la celda de queso, el agente recibe una recompensa de 1.0 puntos.\n",
    "Intentar ingresar a una celda bloqueada (pared) o salir de los límites del laberinto conlleva una penalización severa de -0.75 puntos.\n",
    "Visitar una celda que el agente ya ha visitado anteriormente también recibe una penalización de -0.25 puntos.\n",
    "Finalización del Juego: El juego se considera terminado si el valor acumulado de recompensas del agente cae por debajo de un umbral específico (-0.5 veces el tamaño del laberinto, en este caso -32), lo que sugiere que el agente ha \"perdido su camino\" y cometido demasiados errores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nacho/anaconda3/envs/jupyter/lib/python3.11/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2364 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1501        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014061946 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.0553     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.49        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 3.97        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1334        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015794467 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.369       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0295     |\n",
      "|    value_loss           | 1.5         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1283        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015497504 |\n",
      "|    clip_fraction        | 0.213       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.289       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0316     |\n",
      "|    value_loss           | 1.18        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1240         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0143539235 |\n",
      "|    clip_fraction        | 0.235        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.213        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.163        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0301      |\n",
      "|    value_loss           | 0.55         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Definición del entorno\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "class MazeEnv(gym.Env):\n",
    "    def __init__(self, maze):\n",
    "        super(MazeEnv, self).__init__()\n",
    "        self.maze = maze\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (len(maze) - 1, len(maze[0]) - 1)\n",
    "        self.agent_pos = self.start\n",
    "        self.visited_cells = set()\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(maze), len(maze[0]), 1), dtype=np.float32)\n",
    "        self.total_reward = 0\n",
    "        self.max_moves = 100  # Máximo número de movimientos por episodio\n",
    "        self.moves = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_pos = self.start\n",
    "        self.visited_cells = set()\n",
    "        self.total_reward = 0\n",
    "        self.moves = 0\n",
    "        state = np.zeros((*self.maze.shape, 1), dtype=np.float32)\n",
    "        state[self.agent_pos[0], self.agent_pos[1], 0] = 1.0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.agent_pos\n",
    "\n",
    "        if action == 0:  # Arriba\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 1:  # Abajo\n",
    "            x = min(len(self.maze) - 1, x + 1)\n",
    "        elif action == 2:  # Izquierda\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 3:  # Derecha\n",
    "            y = min(len(maze[0]) - 1, y + 1)\n",
    "\n",
    "        if self.maze[x][y] == 0:  # Obstáculo, penalización de -0.75 puntos\n",
    "            reward = -0.75\n",
    "        elif self.agent_pos == self.goal:  # Objetivo alcanzado, recompensa de 1.0 puntos\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = -0.04  # Penalización por moverse\n",
    "\n",
    "        if self.agent_pos == (x, y):  # Moverse a una posición ya visitada, penalización de -0.25 puntos\n",
    "            reward = -0.25\n",
    "\n",
    "        done = self.agent_pos == self.goal or reward < -0.5 * (len(self.maze) * len(self.maze[0]))\n",
    "        self.agent_pos = (x, y)\n",
    "        self.visited_cells.add(self.agent_pos)\n",
    "        self.total_reward += reward\n",
    "        self.moves += 1\n",
    "\n",
    "        # Crear una matriz que representa el estado del laberinto\n",
    "        state = np.zeros((*self.maze.shape, 1), dtype=np.float32)\n",
    "        state[x, y, 0] = 1.0  # Marcar la posición actual del agente\n",
    "\n",
    "        return state, reward, done, {}\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        if mode != \"console\":\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        for i in range(len(self.maze)):\n",
    "            for j in range(len(self.maze[0])):\n",
    "                if self.agent_pos == (i, j):\n",
    "                    print(\"A\", end=\" \")\n",
    "                elif self.agent_pos == self.goal:\n",
    "                    print(\"G\", end=\" \")\n",
    "                elif self.maze[i][j] == 0:\n",
    "                    print(\"#\", end=\" \")\n",
    "                else:\n",
    "                    print(\".\", end=\" \")\n",
    "            print()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "# Definición del laberinto\n",
    "maze = np.array([\n",
    "    [1., 0., 1., 1., 1., 1., 1., 1.],\n",
    "    [1., 0., 1., 1., 1., 0., 1., 1.],\n",
    "    [1., 1., 1., 1., 0., 1., 0., 1.],\n",
    "    [1., 1., 1., 0., 1., 1., 1., 1.],\n",
    "    [1., 1., 0., 1., 1., 1., 1., 1.],\n",
    "    [1., 1., 1., 0., 1., 0., 0., 0.],\n",
    "    [1., 1., 1., 0., 1., 1., 1., 1.],\n",
    "    [1., 1., 1., 1., 0., 1., 1., 1]\n",
    "])\n",
    "\n",
    "class MazeWrapper(gym.Env):\n",
    "    def __init__(self, maze):\n",
    "        super(MazeWrapper, self).__init__()\n",
    "        self.maze = maze\n",
    "        self.env = MazeEnv(self.maze)\n",
    "\n",
    "        # Definir los espacios de observación y acción en el envoltorio\n",
    "        self.action_space = self.env.action_space\n",
    "        self.observation_space = self.env.observation_space\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def render(self, mode=\"console\"):\n",
    "        self.env.render(mode)\n",
    "\n",
    "# Crear el entorno y envolverlo con DummyVecEnv\n",
    "env = DummyVecEnv([lambda: MazeWrapper(maze)])\n",
    "\n",
    "# Entrenar usando PPO\n",
    "ppo_model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "ppo_model.learn(total_timesteps=10000)  # Ajustar total_timesteps según sea necesario\n",
    "\n",
    "# Guardar el modelo PPO\n",
    "ppo_model.save(\"ppo_maze_model\")\n",
    "\n",
    "# Entrenar usando DQN\n",
    "#dqn_model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "#dqn_model.learn(total_timesteps=10000)  # Ajustar total_timesteps según sea necesario\n",
    "\n",
    "# Guardar el modelo DQN\n",
    "#dqn_model.save(\"dqn_maze_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 19843    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1461     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 19867    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 2093     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 20168    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 3446     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 20271    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 4566     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 20295    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 6041     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 20331    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 7272     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 20304    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 8078     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 20343    |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 9364     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Entrenar usando DQN\n",
    "dqn_model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "dqn_model.learn(total_timesteps=10000)  # Ajustar total_timesteps según sea necesario\n",
    "\n",
    "# Guardar el modelo DQN\n",
    "dqn_model.save(\"dqn_maze_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Entrena agentes en entornos más complejos con librerías de agentes más avanzadas. La idea es que den rienda suelta a su creatividad. Algunas ideas de agentes / entornos:\n",
    "\n",
    "Elegimos utilizar el entorno \"MountainCar-v0\" para realizar esta actividad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fd92bfe8ed0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "model = DQN(\"MlpPolicy\", env)\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fd8526b8690>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DQN(\"MlpPolicy\", gym.make(\"MountainCar-v0\"), tensorboard_log=\"tensorboard/\")\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwXMoJy9oIJT"
   },
   "source": [
    "FIN"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "SD-FSSuKoIJF",
    "uUUkScRxoIJQ"
   ],
   "include_colab_link": true,
   "name": "stable_baselines.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
